category: Text Generation
description: >-
  Generate text using OpenAI's GPT models.
meta:
  source:
    title: 'ChatGPT: Conversational LLM'
    links:
      Website: https://openai.com/chatgpt
      Blog: https://openai.com/blog/chatgpt
      API Docs: https://openai.com/blog/introducing-chatgpt-and-whisper-apis
      Subscription: https://platform.openai.com/apps
      Research: https://openai.com/research
    summary: >-
      ChatGPT is a Large Language Model (LLM) by OpenAI trained to converse with
      the user in a conversational way.
title: ChatGPT (Advanced)
apiNamespace: openai
apiOperationId: createChatCompletion
displayNamespace: openai
displayOperationId: advancedChatGPT
scripts:
  hideExcept:inputs:
    - temperature
    - prompt
    - model
    - instruction
    - function
    - top_p
  hideExcept:outputs:
    - answer_text
    - function_call
    - function_arguments_string
    - function_arguments
    - function_name
    - raw_output
inputs:
  temperature:
    description: >-
      The randomness regulator, higher for more creativity, lower for more
      structured, predictable text.
    minimum: 0
    maximum: 1
    step: 0.1
  function:
    type: object
    customSocket: object
  functions:
    type: array
    customSocket: object
    socketOpts:
      array: true
    scripts:
      jsonata: >-
        $exists(function) ? [function] : None
      delete:
        - function
  instruction:
    default: You are a helpful bot.  You can help people by answering their questions.
    type: string
    customSocket: text
  prompt:
    required: true
    type: string
    customSocket: text
    default: Create a novel AI koan.
  function_call:
    type: string
    scripts:
      jsonata: >-
        functions.length > 0 ? "auto" : None
  messages:
    scripts:
      jsonata: >-
        [{"role":"system", "content": $string(instruction) }, {"role":"user",
        "content": $string(prompt) }]
      delete:
        - prompt
        - instruction
  model:
    title: Model
    default: gpt-3.5-turbo
    choices:
      block: openai.getGPTModels
      cache: user
      map:
        root: models
        filter:
          value: "^(?!.*gpt-4-vision-preview).*$"
  top_p:
    description: >-
      Implements nucleus sampling, where the model considers only tokens whose cumulative probability exceeds the specified `top_p` threshold.
      Like `temperature`, this controls the diversity of the model's output.
    default: 1
    minimum: 0
    maximum: 1
    step: 0.05
outputs:
  answer_text:
    scripts:
      jsonata: choices[0].message.content
    type: string
    customSocket: text
  function_call:
    scripts:
      jsonata: >-
        {'name': choices[0].message.function_call.name, 'arguments':'string' ?
        $eval(choices[0].message.function_call.arguments) :
        choices[0].message.function_call.arguments}
    type: object
    customSocket: object
  function_arguments_string:
    description: This is a string representation of the function arguments.
    scripts:
      jsonata: $string(choices[0].message.function_call.arguments)
    type: string
    customSocket: text
  function_arguments:
    description: >-
      This is an object representation of the function arguments. However, note
      that this may fail as the output is often not formatted properly.
    scripts:
      jsonata: $eval(function_arguments_string)
    type: object
    customSocket: object
  function_name:
    scripts:
      jsonata: choices[0].message.function_call.name
    type: string
    customSocket: text
  raw_output:
    type: object
    scripts:
      jsonata: choices[0]
