openapi: 3.1.0
info:
  title: OpenAI-Compatible API
  version: 2.0.0
paths:
  /v1/chat/completions:
    post:
      operationId: createChatCompletion
      tags:
        - OpenAI
        - LM-Studio
      summary: Creates a model response for the given chat conversation.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                model:
                  type: string
                  required: true
                messages:
                  description: A list of messages comprising the conversation so far.
                  type: array
                  minItems: 1
                  items:
                    role:
                      type: string
                      enum:
                        - system
                        - user
                        - assistant
                        - function
                      description: >-
                        The role of the messages author. One of `system`, `user`,
                        `assistant`, or `function`.
                    content:
                      type: string
                      nullable: true
                      description: >-
                        The contents of the message. `content` is required for all
                        messages, and may be null for assistant messages with function
                        calls.
                    name:
                      type: string
                      description: >-
                        The name of the author of this message. `name` is required if
                        role is `function`, and it should be the name of the function
                        whose response is in the `content`. May contain a-z, A-Z, 0-9,
                        and underscores, with a maximum length of 64 characters.
                    function_call:
                      type: object
                      description: >-
                        The name and arguments of a function that should be called, as
                        generated by the model.
                      properties:
                        name:
                          type: string
                          description: The name of the function to call.
                        arguments:
                          type: string
                          description: >-
                            The arguments to call the function with, as generated by the
                            model in JSON format. Note that the model does not always
                            generate valid JSON, and may hallucinate parameters not
                            defined by your function schema. Validate the arguments in
                            your code before calling your function.
                      required:
                        - name
                        - arguments
                    required:
                      - role
                      - content
                functions:
                  description: A list of functions the model may generate JSON inputs for.
                  type: array
                  minItems: 1
                  items:
                    name:
                      type: string
                      description: >-
                        The name of the function to be called. Must be a-z, A-Z, 0-9, or
                        contain underscores and dashes, with a maximum length of 64.
                    description:
                      type: string
                      description: >-
                        A description of what the function does, used by the model to
                        choose when and how to call the function.
                    parameters:
                      $ref: '#/components/schemas/ChatCompletionFunctionParameters'
                  required:
                    - name
                    - parameters
                function_call:
                  description: >-
                    Controls how the model responds to function calls. "none" means the
                    model does not call a function, and responds to the end-user. "auto"
                    means the model can pick between an end-user or calling a function.
                    Specifying a particular function via `{"name":\ "my_function"}`
                    forces the model to call that function. "none" is the default when
                    no functions are present. "auto" is the default if functions are
                    present.
                  oneOf:
                    - type: string
                      enum:
                        - none
                        - auto
                      name:
                        type: string
                        description: The name of the function to call.
                      required:
                        - name
                temperature:
                  type: number
                  minimum: 0
                  maximum: 2
                  default: 1
                  example: 1
                  description: >-
                    The randomness regulator, higher for more creativity, lower for more
                    structured, predictable text.
                top_p:
                  type: number
                  minimum: 0
                  maximum: 1
                  default: 1
                  example: 1
                  description: >-
                    An alternative to sampling with temperature, called nucleus
                    sampling, where the model considers the results of the tokens with
                    top_p probability mass. So 0.1 means only the tokens comprising the
                    top 10% probability mass are considered. We generally recommend
                    altering this or `temperature` but not both.
                'n':
                  type: integer
                  minimum: 1
                  maximum: 128
                  default: 1
                  example: 1
                  description: How many chat completion choices to generate for each input message.
                stream:
                  description: If set, partial message deltas will be sent, like in ChatGPT.
                  default: false
                stop:
                  description: |
                    Up to 4 sequences where the API will stop generating further tokens.
                  default: null
                  oneOf:
                    - type: string
                      nullable: true
                    - type: array
                      minItems: 1
                      maxItems: 4
                      items:
                        type: string
                max_tokens:
                  description: >
                    The maximum number of [tokens](/tokenizer) to generate in the chat
                    completion.


                    The total length of input tokens and generated tokens is limited by
                    the model's context length. [Example Python
                    code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
                    for counting tokens.
                  default: inf
                  type: integer
                presence_penalty:
                  type: number
                  default: 0
                  minimum: -2
                  maximum: 2
                  nullable: true
                  description: >-
                    Number between -2.0 and 2.0. Positive values penalize new tokens
                    based on whether they appear in the text so far, increasing the
                    model's likelihood to talk about new topics.
                frequency_penalty:
                  type: number
                  default: 0
                  minimum: -2
                  maximum: 2
                  nullable: true
                  description: >-
                    Number between -2.0 and 2.0. Positive values penalize new tokens
                    based on their existing frequency in the text so far, decreasing the
                    model's likelihood to repeat the same line verbatim.
                logit_bias:
                  type: object
                  x-oaiTypeLabel: map
                  default: null
                  nullable: true
                  additionalProperties:
                    type: integer
                  description: >
                    Modify the likelihood of specified tokens appearing in the
                    completion. Accepts a json object that maps tokens (specified by
                    their token ID in the GPT tokenizer) to an associated bias value
                    from -100 to 100.
              required:
                - model
                - messages
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  id:
                    type: string
                    description: A unique identifier for the chat completion.
                  object:
                    type: string
                    description: The object type, which is always `chat.completion`.
                  created:
                    type: integer
                    description: A unix timestamp of when the chat completion was created.
                  model:
                    type: string
                    description: The model used for the chat completion.
                  choices:
                    type: array
                    description: >-
                      A list of chat completion choices. Can be more than one if
                      `n` is greater than 1.
                    items:
                      type: object
                      required:
                        - index
                        - message
                        - finish_reason
                      properties:
                        index:
                          type: integer
                          description: The index of the choice in the list of choices.
                        message:
                          description: A chat completion message generated by the model.
                          properties:
                            role:
                              type: string
                              enum:
                                - system
                                - user
                                - assistant
                                - function
                              description: The role of the author of this message.
                            content:
                              type: string
                              description: The contents of the message.
                              nullable: true
                            function_call:
                              type: object
                              description: >-
                                The name and arguments of a function that should
                                be called, as generated by the model.
                              properties:
                                name:
                                  type: string
                                  description: The name of the function to call.
                                arguments:
                                  type: string
                                  description: >-
                                    The arguments to call the function with, as
                                    generated by the model in JSON format. Note
                                    that the model does not always generate
                                    valid JSON, and may hallucinate parameters
                                    not defined by your function schema.
                                    Validate the arguments in your code before
                                    calling your function.
                              required:
                                - name
                                - arguments
                          required:
                            - role
                            - content
                        finish_reason:
                          type: string
                          description: >-
                            The reason the model stopped generating tokens. This
                            will be `stop` if the model hit a natural stop point
                            or a provided stop sequence, `length` if the maximum
                            number of tokens specified in the request was
                            reached, or `function_call` if the model called a
                            function.
                          enum:
                            - stop
                            - length
                            - function_call
